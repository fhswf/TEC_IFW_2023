{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9c02df-4507-4656-9acb-f65043deebb4",
   "metadata": {},
   "source": [
    "# Exercise: Reinforcement Learning with a Policy Network\n",
    "\n",
    "![CartPole](https://www.gymlibrary.dev/_images/cart_pole.gif)\n",
    "\n",
    "[CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) is a control problem. The goal is to balance a pendulum mounted on a cart.\n",
    "\n",
    "It's well suited to start with as the state space has only four dimensions\n",
    "\n",
    "- position of the cart,\n",
    "- speed of the cart,\n",
    "- angle of the pendulum, and\n",
    "- angular velocity of the pendulum\n",
    "\n",
    "and there are only two possible actions\n",
    "\n",
    "- push the cart to the right,\n",
    "- push the cart to the left.\n",
    "\n",
    "\n",
    "The first exercise is to balance the pendulum with a simple *policy network*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8ecf4-4cee-454d-9671-9f10c8dfabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jdc\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a5271-2699-4cac-8659-1b05d9eef977",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "First we create an *environment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd42433-74be-4913-8fe5-13f8f58f2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gymnasium import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for split class definitions\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3891a0-f9f4-453a-8f3c-c924b0e5c264",
   "metadata": {},
   "source": [
    "## Rendering in a Jupyter notebook\n",
    "\n",
    "Normally you need a \"real\" application for rendering. We make do here with `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6f9f5-9cb4-4fd1-b50e-3b610bb8c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, img):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32bb37-6b05-4814-a415-717f428d7fdd",
   "metadata": {},
   "source": [
    "## Baseline: `RandomPolicy`\n",
    "\n",
    "The following policy simply makes random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857b711-b64f-4c46-bde6-cf1a85883316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    \n",
    "    def __call__(self, observation):\n",
    "        return random.choice([0, 1])\n",
    "    \n",
    "    def update(self, *args):\n",
    "        # Do nothing\n",
    "        pass\n",
    "    \n",
    "    def init_game(self, observation):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d47b6-8d9f-4de3-bb1a-ab6186e48c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(policy, episodes=2000, do_render = False, seed=100):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if do_render:\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    else:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    policy.init_game(observation)\n",
    "\n",
    "    if do_render:\n",
    "        plt.ion()\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(env.render())\n",
    "   \n",
    "    status = {}\n",
    "    episode = 0\n",
    "    status['steps'] = 0\n",
    "    status['episode_reward'] = 0\n",
    "    status['average_reward'] = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "\n",
    "    with tqdm(total=episodes) as pbar:\n",
    "        pbar.set_postfix(status)\n",
    "        while True:\n",
    "            try:\n",
    "                action = policy(observation)\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                status['steps'] += 1\n",
    "                status['episode_reward'] += reward\n",
    "                if do_render:\n",
    "                    render(env, img)\n",
    "                policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "                if terminated or status['steps'] > 1000:\n",
    "                    episode += 1\n",
    "                    if episode > pbar.total:\n",
    "                        break\n",
    "                    total_reward += status['episode_reward']\n",
    "                    status['average_reward'] = 0.05 * status['episode_reward'] + (1 - 0.05) * status['average_reward']\n",
    "                    if status['average_reward'] > env.spec.reward_threshold:\n",
    "                        print(f\"Solved! Running reward is now {status['average_reward']} and \"\n",
    "                              f\"the last episode runs to {status['steps']} time steps!\")\n",
    "                        break\n",
    "\n",
    "                    pbar.set_postfix(status, refresh=episode % 10 == 0)\n",
    "                    pbar.update()\n",
    "                    status['steps'] = 0\n",
    "                    \n",
    "                    status['episode_reward'] = 0\n",
    "                    observation, info = env.reset()\n",
    "                    policy.init_game(observation)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00b3cd-23d3-49ff-b0ff-460a24abba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = RandomPolicy()\n",
    "play_game(policy, episodes=10, do_render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27f421-0f45-4cc6-a571-da115ac1380f",
   "metadata": {},
   "source": [
    "## Task 1: Policy Network\n",
    "\n",
    "For our first model, we need a policy network that translates the four-dimesional state into two actions. \n",
    "The model should look like this:\n",
    "\n",
    "1. A `Linear` layer with `hidden_size` as target dimension and `ReLU` as activation function.\n",
    "2. A `Linear` layer with `n_actions` as target dimension and `Softmax` as activation function used as a `policy_head` \n",
    "   (this is a more general approach than the one I used in my presentation). \n",
    "   \n",
    "We use two separate parts because we want to add another output \"head\" later on for an *actor critic approach*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25a7c1-da09-445e-8366-2a682e8c68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=32, n_actions=2):\n",
    "        super().__init__()\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE HERE\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755b053-9a8b-4eae-8f79-f3766db18c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob'])\n",
    "    \n",
    "class SimplePolicy:\n",
    "    \n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        # Two possible actions 0, 1\n",
    "        self.ACTIONS = [0, 1]       \n",
    "        self.net = PolicyNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755494d8-2ec3-4683-91c9-8d8337be9890",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "The function `__call__(self, observation)` calculates the action as follows:\n",
    "\n",
    "- The probabilities `probs` are calculated with `PolicyNet` (here accessible as `self.net`),\n",
    "- a suitable probability distribution is generated with `torch.distrib.Categorial` and an action is diced with `sample()`,\n",
    "- in `self.memory` the logarithm of the probability (`m.log_prob(acttion)`) is stored (for later training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd9f5d-20f7-4cec-b8cd-6146cdbebfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SimplePolicy   \n",
    "      \n",
    "    def __call__(self, observation):\n",
    " \n",
    "        probs = ### YOUR CODE HERE\n",
    "        m = ### YOUR CODE HERE\n",
    "        action = ### YOUR CODE HERE\n",
    "        \n",
    "        self.memory.append(SavedAction(m.log_prob(action)))\n",
    "        \n",
    "        return self.ACTIONS[action.item()]\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        \n",
    "    def discount_rewards(self, r):\n",
    "        discounted = torch.zeros(len(r))\n",
    "        summe = 0\n",
    "        for t in reversed(range(0, len(r))):\n",
    "            summe = summe * self.gamma + r[t]\n",
    "            discounted[t] = summe\n",
    "        return discounted\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1aa18d-b1df-454c-b6f7-3b9748e5e028",
   "metadata": {},
   "source": [
    "### Update of the model\n",
    "\n",
    "Training takes place at the end of each game episode:\n",
    "\n",
    "- First, the *discounted rewards* are calculated,\n",
    "- these are scaled so that they are normally distributed.\n",
    "- The loss of the policy is given as `- reward * log_prob`,\n",
    "- an `optimizer.step()` is performed with the sum of the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45dd630-7ad4-4cd3-8f42-b03465740535",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to SimplePolicy  \n",
    "    \n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "            \n",
    "            policy_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                policy_losses.append(### YOUR CODE HERE ###)\n",
    "                \n",
    "            loss = torch.stack(policy_losses).sum()\n",
    "            ### YOUR CODE HERE\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb88408-d74a-4c1f-a8a0-00ad3f065052",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SimplePolicy()\n",
    "play_game(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856b69e-a73f-4341-93a2-2b2c7c5787c8",
   "metadata": {},
   "source": [
    "## Task 2: Actor-Critic-Model\n",
    "\n",
    "For the actor-critic model, our network is given another output `critic`, which is to provide an estimate of the value of a state.\n",
    "This creates better feedback for the policy and improves training.\n",
    "\n",
    "The critic head also receives its input from the hidden layer. Since the feedback value can be any number, *there should be no activation function* for the critic head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e08835-f835-4e51-b342-673eead6e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=128, n_actions=2):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE HERE\n",
    "        probs = self.policy(x)\n",
    "        value = self.critic(x)\n",
    "        return probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bdf14-3f21-41e5-8167-e83a26e84314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "    \n",
    "class ACPolicy:\n",
    "    \n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        # Two possible actions 0, 1\n",
    "        self.ACTIONS = [0, 1]       \n",
    "        self.net = ActorCriticNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba5c79-b35e-4088-9cd2-0118e5ce2878",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "The strategy now also calculates the value of the state using the network and saves it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52797731-ad23-4c61-9208-9ec426796d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to ACPolicy  \n",
    "\n",
    "   def __call__(self, observation):\n",
    " \n",
    "        probs, value = ### YOUR CODE HERE\n",
    "        m = ### YOUR CODE HERE\n",
    "        action = ### YOUR CODE HERE\n",
    "        \n",
    "        self.memory.append(SavedAction(m.log_prob(action), value))\n",
    "        \n",
    "        return self.ACTIONS[action.item()]\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1060479-cadb-4532-8de0-003bc3fdffe9",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The loss function now consists of two parts:\n",
    "\n",
    "1. The `policy_loss` sums `log_prob * advantage`, where `advantage = discounted_reward - value`. \n",
    "   This difference is also called the *temporal difference`.\n",
    "2. The `value_loss` sums the difference between `value` and discounted reward. \n",
    "   Mostly `F.smooth_l1_loss` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ceee97-d8aa-4928-b3d3-05f72f30205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "  %%add_to ACPolicy  \n",
    "        \n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "            \n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                ### YOUR CODE HERE\n",
    "               \n",
    "            self.optimizer.zero_grad()\n",
    "            ### YOUR CODE HERE\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc6c96-3282-4238-868a-f420c4aed689",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ACPolicy()\n",
    "play_game(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6ff7d-c9ef-48b1-bd6f-79f2a7a5a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(policy, do_render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036415e-bd36-49c5-bf4d-38b1c8c748ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

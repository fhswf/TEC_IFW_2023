{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d6114d-1a7d-44d3-9eab-fbdd56eef690",
   "metadata": {},
   "source": [
    "# Solving Pong with a policy network\n",
    "\n",
    "This example notebook solves Pong with a very simple *Policy Network* and the Pytorch environment.\n",
    "\n",
    "The image preprocessing is copied from [Andrej Karpathy's gist](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5) which implements a similar network and gradient descent using low-level `numpy` code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbacca6-3857-473f-aa62-231ef2721392",
   "metadata": {},
   "source": [
    "We start by installing gymnasium (the officialy maintained version of OpenAI gym).\n",
    "Please be aware that the download of the Atari ROMs is subject to specific terms (education and research use only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438cda11-7f02-4b02-a4c1-b31df4a58635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[atari] in /opt/conda/lib/python3.10/site-packages (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (1.22.4)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (4.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (2.2.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (0.0.1)\n",
      "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (0.2.1)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /opt/conda/lib/python3.10/site-packages (from gymnasium[atari]) (0.4.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (4.64.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (8.1.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (2.28.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (0.5.5)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /opt/conda/lib/python3.10/site-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (5.9.0)\n",
      "Requirement already satisfied: libtorrent in /opt/conda/lib/python3.10/site-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (2022.6.15.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[atari]) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[atari] gymnasium[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afaac99b-c1f7-4561-beff-450a8dd57c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "from gymnasium import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# enable simple browser-based rendering\n",
    "do_render = True\n",
    "\n",
    "# create the Pong environment\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c79fe-6a98-4b2a-8197-8577a5c6ad9d",
   "metadata": {},
   "source": [
    "## Rendering using `matplotlib`\n",
    "\n",
    "In this Jupyter environment we can't use the usual rendering routines as the depend on some kind of display.\n",
    "Since the screen is very small, we can just render the bitmap using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962e5704-6cd3-4d41-9480-7a7b662d9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env):\n",
    "    if do_render:\n",
    "        img.set_data(env.render())\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e02b8f-c58c-4b7a-a154-237de48d7028",
   "metadata": {},
   "source": [
    "## Preprocessing the images\n",
    "\n",
    "The following code preprocesses the images, removing unnecessary information. This help speeding up the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b99c4a58-9db4-4f26-9fa9-d4010bc6c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\" \n",
    "    Preprocess 210x160x3 uint8 frame into 6000 (75x80) 1D float vector \n",
    "    \n",
    "    The preprocessing code was copied from \n",
    "    [Andrej Karpathy's gist](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "    \"\"\"\n",
    "    x = x[35:185]       # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n",
    "    x = x[::2, ::2, 0]  # downsample by factor of 2.\n",
    "    x[x == 144] = 0     # erase background (background type 1)\n",
    "    x[x == 109] = 0     # erase background (background type 2)\n",
    "    \n",
    "    # everything else (paddles, ball) just set to 1. this makes the image grayscale effectively\n",
    "    x[x != 0] = 1\n",
    "    \n",
    "    return torch.tensor(x).float().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c70aab-34c0-46c6-8c37-44a7e2b88e90",
   "metadata": {},
   "source": [
    "## Defining a policy making random moves\n",
    "\n",
    "This 'stupid' policy just makes random moves with the paddle and does not learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b62bf4-49a9-4c97-8517-78d293dda64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    \n",
    "    def __call__(self, observation):\n",
    "        return random.choice([0, 2, 3])\n",
    "    \n",
    "    def update(self, *args):\n",
    "        # Do nothing\n",
    "        pass\n",
    "    \n",
    "    def init_game(self, observation):\n",
    "        pass\n",
    "    \n",
    "policy = RandomPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023972c-21c5-4e0c-9fc2-45d67e55fab4",
   "metadata": {},
   "source": [
    "## The main loop for playing and training\n",
    "\n",
    "The following code plays 10.000 games with a `policy`.\n",
    "\n",
    "If you use this for training, be sure to set `do_render` to `False` to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f2ba5d-799d-481d-b948-9840822c0bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "observation = preprocess(observation)\n",
    "policy.init_game(observation)\n",
    "\n",
    "plt.ion()\n",
    "plt.axis('off')\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "with tqdm(total=10000) as pbar:\n",
    "    while True:\n",
    "        try:\n",
    "            action = policy(observation)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            observation = preprocess(observation)\n",
    "            render(env)\n",
    "            policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                pbar.update()\n",
    "                observation, info = env.reset()\n",
    "                observation = preprocess(observation)\n",
    "                policy.init_game(observation)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f18c7-0cee-4d28-92e8-5ba095b65791",
   "metadata": {},
   "source": [
    "## A simple policy network\n",
    "\n",
    "The following code defines a simple neural network with a single hidden layer an a single output neuron.\n",
    "The activation of the output neuron is interpreted as the probability of moving the paddle UP.\n",
    "\n",
    "Learning takes place in the `update()` function which defines a simple loss function based on discounted rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178ead39-20a9-49e2-8db5-40b5a9e61cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Preprocessed input is 75 * 80\n",
    "        self.fc1 = nn.Linear(75 * 80, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class SimplePolicy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Two possible actions (UP, DOWN)\n",
    "        self.ACTIONS = [2, 3]       \n",
    "        self.net = PolicyNetwork()\n",
    "        self.optimizer = torch.optim.RMSprop(self.net.parameters(), lr=0.001)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "    def __call__(self, observation):\n",
    "        diff = observation - self.last_observation\n",
    "        p = self.net(diff)\n",
    "        m = Bernoulli(p)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.diffs.append(diff)\n",
    "        self.actions.append(action)\n",
    "        self.last_observation = observation\n",
    "        \n",
    "        return self.ACTIONS[int(action.item())]\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.last_observation = observation\n",
    "        self.diffs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        \n",
    "    def discount_rewards(self, r):\n",
    "        discounted = torch.zeros(len(r))\n",
    "        summe = 0\n",
    "        for t in reversed(range(0, len(r))):\n",
    "            if r[t] != 0:\n",
    "                # reset the sum, since this was a game boundary (pong specific!)\n",
    "                summe = 0\n",
    "            summe = summe * self.gamma + r[t]\n",
    "            discounted[t] = summe\n",
    "        return discounted\n",
    "        \n",
    "    def update(self, observation, reward, terminated, truncated, info, pbar):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "            pbar.set_postfix({ 'games': self.games, 'mean_reward': self.mean_reward,'total_reward': self.total_reward})\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = self.discount_rewards(self.rewards)\n",
    "            discounted -= torch.mean(discounted)\n",
    "            discounted /= torch.std(discounted)\n",
    "            \n",
    "            x = torch.stack(self.diffs)\n",
    "            actions = torch.FloatTensor(self.actions)\n",
    "            p = self.net(x)\n",
    "            loss = 0\n",
    "            for i in range(x.size(0)):\n",
    "                m = Bernoulli(p[i])\n",
    "                loss += -m.log_prob(actions[i]) * discounted[i]\n",
    "                \n",
    "            loss.backward()    \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)\n",
    "\n",
    "policy = SimplePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cb76f-b95c-465e-b898-74ba34982bc3",
   "metadata": {},
   "source": [
    "## Trying a trained model\n",
    "\n",
    "Try loading the weights of a pretrained model and let it play ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cd86755-57bc-4eb5-974b-c4009ba1e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.load('policy_model_trained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4b6af-78ab-4edb-a449-31fe87aea7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

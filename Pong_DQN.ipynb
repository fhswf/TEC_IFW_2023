{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20f2cab-1ee5-4c0b-9d63-addece2efa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym[accept-rom-license,atari]\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (1.22.4)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2\n",
      "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Collecting ale-py~=0.8.0\n",
      "  Using cached ale_py-0.8.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.3.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (5.9.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.28.1)\n",
      "Collecting AutoROM.accept-rom-license\n",
      "  Using cached AutoROM.accept_rom_license-0.4.2-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.1.1)\n",
      "Installing collected packages: gym-notices, gym, ale-py, AutoROM.accept-rom-license, autorom\n",
      "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.8.0 autorom-0.4.2 gym-0.26.2 gym-notices-0.0.8\n",
      "Collecting moviepy\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from moviepy) (1.22.4)\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.21.3)\n",
      "Collecting proglog<=1.0.0\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.64.1)\n",
      "Collecting imageio-ffmpeg>=0.2.0\n",
      "  Using cached imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.6.15.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.11)\n",
      "Installing collected packages: proglog, imageio-ffmpeg, decorator, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio-ffmpeg-0.4.7 moviepy-1.0.3 proglog-0.1.10\n",
      "Collecting pysdl2\n",
      "  Using cached PySDL2-0.9.14-py3-none-any.whl\n",
      "Installing collected packages: pysdl2\n",
      "Successfully installed pysdl2-0.9.14\n",
      "Collecting pyvirtualdisplay\n",
      "  Using cached PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyvirtualdisplay\n",
      "Successfully installed pyvirtualdisplay-3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari,accept-rom-license]\n",
    "!pip install moviepy\n",
    "!pip install pysdl2\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc32e69c-6cd1-4fae-b0b0-8eb8bc0baef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.22.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afaac99b-c1f7-4561-beff-450a8dd57c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n",
      "/opt/conda/lib/python3.10/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/jovyan/work/Deep_Learning/video/pong-base folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "# record the game as as an mp4 file\n",
    "env = wrappers.RecordVideo(env, 'video/pong-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9077e2fd-1b30-4e5d-a71b-fa8f77823656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303ee66b-58f1-4436-9096-66901e1d6695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob, info = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, truncated, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, truncated, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        super(EpisodicLifeEnv, self).__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "        self.was_real_reset = False\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs, info = self.env.reset()\n",
    "            self.was_real_reset = True\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _, info = self.env.step(0)\n",
    "            self.was_real_reset = False\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs, info\n",
    "    \n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs, info = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs, info\n",
    "\n",
    "    \n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        super(NoopResetEnv, self).__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset()\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = np.random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        info = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _, info = self.env.step(0)\n",
    "            if done:\n",
    "                obs, info = self.env.reset()\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca7cd42-3672-4c19-9444-4f92515983a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env, stack_frames=True, episodic_life=True):\n",
    "    if episodic_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    #if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "    #    env = FireResetEnv(env)\n",
    "\n",
    "    env = WarpFrame(env)\n",
    "    if stack_frames:\n",
    "        env = FrameStack(env, 4)\n",
    "  \n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f601ecc-c1df-426d-8e6a-548ef60983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962e5704-6cd3-4d41-9480-7a7b662d9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0782329-cd11-4bab-bff2-dbc09240a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_actions=14):\n",
    "        \"\"\"\n",
    "        Initialize Deep Q Network\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_actions (int): number of outputs\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        # self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.head = nn.Linear(512, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88156d3-935d-4f0f-a007-b8cd60c517f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026daa27-f114-4092-94b6-abf8a83a4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQNPolicy:\n",
    "    \n",
    "    BATCH_SIZE = 16636\n",
    "    GAMMA = 0.99\n",
    "    EPS_START = 1\n",
    "    EPS_END = 0.02\n",
    "    EPS_DECAY = 1000000\n",
    "    TARGET_UPDATE = 1000\n",
    "    RENDER = False\n",
    "    INITIAL_MEMORY = 10000\n",
    "    MEMORY_SIZE = 10 * INITIAL_MEMORY\n",
    "    \n",
    "    def __init__(self, lr=1e-4):\n",
    "        self.n_actions = 4\n",
    "        self.steps_done = 0\n",
    "        self.mean_reward = None\n",
    "        self.policy_net = DQN(n_actions=self.n_actions).to(device)\n",
    "        self.target_net = DQN(n_actions=self.n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.memory = ReplayMemory(DQNPolicy.MEMORY_SIZE)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "    def get_state(self, obs):\n",
    "        state = np.array(obs)\n",
    "        state = state.transpose((2, 0, 1))\n",
    "        state = torch.from_numpy(state)\n",
    "        return state.unsqueeze(0)\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        state = self.get_state(observation)\n",
    "        sample = random.random()\n",
    "        eps_threshold = DQNPolicy.EPS_END + (DQNPolicy.EPS_START - DQNPolicy.EPS_END)* math.exp(-1. * self.steps_done / DQNPolicy.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state.to(device)).max(1)[1].view(1,1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.state = self.get_state(observation)\n",
    "        self.total_reward = 0.0\n",
    "    \n",
    "    def update(self, obs, reward, terminated, truncated, info, pbar):\n",
    "        self.total_reward += reward\n",
    "        if not terminated:\n",
    "            self.next_state = self.get_state(obs)\n",
    "        else:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "            pbar.set_postfix({'total_reward': self.total_reward, 'mean_reward': self.mean_reward, 'steps': self.steps_done})\n",
    "            self.next_state = None\n",
    "            \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        self.memory.push(self.state, action.to('cpu'), self.next_state, reward.to('cpu'))\n",
    "        self.state = self.next_state\n",
    "\n",
    "        if terminated and self.steps_done > DQNPolicy.INITIAL_MEMORY:\n",
    "            self.optimize_model()\n",
    "\n",
    "        if self.steps_done % DQNPolicy.TARGET_UPDATE == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())      \n",
    "            \n",
    "        if self.steps_done % 100_000 == 0:\n",
    "            self.save(f'model_{self.steps_done}.pt')\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < DQNPolicy.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(DQNPolicy.BATCH_SIZE)\n",
    "        \"\"\"\n",
    "        zip(*transitions) unzips the transitions into\n",
    "        Transition(*) creates new named tuple\n",
    "        batch.state - tuple of all the states (each state is a tensor)\n",
    "        batch.next_state - tuple of all the next states (each state is a tensor)\n",
    "        batch.reward - tuple of all the rewards (each reward is a float)\n",
    "        batch.action - tuple of all the actions (each action is an int)    \n",
    "        \"\"\"\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        actions = tuple((map(lambda a: torch.tensor([[a]], device=device), batch.action))) \n",
    "        rewards = tuple((map(lambda r: torch.tensor([r], device=device), batch.reward))) \n",
    "\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=device, dtype=torch.bool)\n",
    "\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None]).to(device)\n",
    "\n",
    "\n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        action_batch = torch.cat(actions)\n",
    "        reward_batch = torch.cat(rewards)\n",
    "\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = torch.zeros(DQNPolicy.BATCH_SIZE, device=device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * DQNPolicy.GAMMA) + reward_batch\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "        \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict()) \n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        if \"mean_reward\" in checkpoint:\n",
    "            self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'steps_done': self.steps_done,\n",
    "                    'model_state_dict': self.policy_net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e63ce68-4b97-43dc-a817-8694357d2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQNPolicy()\n",
    "policy.load(\"model_8500000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1783180-33be-459a-b839-d1bd100c5ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "policy.init_game(observation)\n",
    "\n",
    "plt.ion()\n",
    "plt.axis('off')\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "with tqdm(total=10000) as pbar:\n",
    "    while True:\n",
    "        try:\n",
    "            action = policy(observation)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            render(env)\n",
    "            policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                pbar.update()\n",
    "                observation, info = env.reset()\n",
    "                policy.init_game(observation)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1ecfeabc-4fdb-4371-8cde-e1c3a32e7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4b6af-78ab-4edb-a449-31fe87aea7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
